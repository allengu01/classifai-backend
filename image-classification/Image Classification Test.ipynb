{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "032b3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket_name = 'classifai-backend'\n",
    "dataset_name = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36facb2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, image_uris\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "training_image = image_uris.retrieve('image-classification', sess.boto_region_name, version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44ea54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=/tmp\n",
      "env: S3_DATA_BUCKET_NAME=classifai-backend\n",
      "env: DATASET_NAME=test\n",
      "env: IM2REC=/home/ec2-user/anaconda3/envs/mxnet_latest_p37/cpu/lib/python3.7/site-packages/mxnet/tools/im2rec.py\n"
     ]
    }
   ],
   "source": [
    "# Find im2rec in our environment and set up some other vars in our environemnt\n",
    "\n",
    "base_dir='/tmp'\n",
    "\n",
    "%env BASE_DIR=$base_dir\n",
    "%env S3_DATA_BUCKET_NAME = $data_bucket_name\n",
    "%env DATASET_NAME = $dataset_name\n",
    "\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter((lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c072751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull our images from S3\n",
    "!aws s3 sync s3://$S3_DATA_BUCKET_NAME/public/$DATASET_NAME $BASE_DIR/$DATASET_NAME --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d26acd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LST files\n",
      "Label classes:\n",
      "clip 0\n",
      "mouse 1\n",
      "pen 2\n",
      "Creating RecordIO files\n",
      "Creating .rec file from /tmp/test_train.lst in /tmp\n",
      "time: 0.05218958854675293  count: 0\n",
      "Creating .rec file from /tmp/test_test.lst in /tmp\n",
      "time: 0.048287153244018555  count: 0\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 787K Jul  6 20:24 test_test.rec\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.8M Jul  6 20:24 test_train.rec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use the IM2REC script to convert our images into RecordIO files\n",
    "\n",
    "# Clean up our working dir of existing LST and REC files\n",
    "cd $BASE_DIR\n",
    "rm *.rec\n",
    "rm *.lst\n",
    "\n",
    "# First we need to create two LST files (training and test lists), noting the correct label class for each image\n",
    "# We'll also save the output of the LST files command, since it includes a list of all of our label classes\n",
    "echo \"Creating LST files\"\n",
    "python $IM2REC --list --recursive --pass-through --test-ratio=0.3 --train-ratio=0.7 $DATASET_NAME $DATASET_NAME > ${DATASET_NAME}_classes\n",
    "\n",
    "echo \"Label classes:\"\n",
    "cat ${DATASET_NAME}_classes\n",
    "\n",
    "# Then we create RecordIO files from the LST files\n",
    "echo \"Creating RecordIO files\"\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_train.lst $DATASET_NAME\n",
    "python $IM2REC --num-thread=4 ${DATASET_NAME}_test.lst $DATASET_NAME\n",
    "ls -lh *.rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb42244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-east-2-964137047091/test/train/test_train.rec\n",
      "delete: s3://sagemaker-us-east-2-964137047091/test/validation/test_test.rec\n",
      "upload: ../../../../../tmp/test_train.rec to s3://sagemaker-us-east-2-964137047091/test/train/test_train.rec\n",
      "upload: ../../../../../tmp/test_test.rec to s3://sagemaker-us-east-2-964137047091/test/validation/test_test.rec\n"
     ]
    }
   ],
   "source": [
    "# Upload our train and test RecordIO files to S3 in the bucket that our sagemaker session is using\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "s3train_path = 's3://{}/{}/train/'.format(bucket, dataset_name)\n",
    "s3validation_path = 's3://{}/{}/validation/'.format(bucket, dataset_name)\n",
    "\n",
    "# Clean up any existing data\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/train --recursive\n",
    "!aws s3 rm s3://{bucket}/{dataset_name}/validation --recursive\n",
    "\n",
    "# Upload the rec files to the train and validation channels\n",
    "!aws s3 cp /tmp/{dataset_name}_train.rec $s3train_path\n",
    "!aws s3 cp /tmp/{dataset_name}_test.rec $s3validation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8972265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3train_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3validation_path, \n",
    "    distribution='FullyReplicated', \n",
    "    content_type='application/x-recordio', \n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d6b8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, dataset_name)\n",
    "\n",
    "image_classifier = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.p2.xlarge',\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d5b7775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained_model': 1,\n",
       " 'image_shape': '3,224,224',\n",
       " 'num_classes': 3,\n",
       " 'num_training_samples': 21,\n",
       " 'learning_rate': 0.001,\n",
       " 'mini_batch_size': 5}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes=! ls -l {base_dir}/{dataset_name} | wc -l\n",
    "num_classes=int(num_classes[0]) - 1\n",
    "\n",
    "num_training_samples=! cat {base_dir}/{dataset_name}_train.lst | wc -l\n",
    "num_training_samples = int(num_training_samples[0])\n",
    "\n",
    "# Learn more about the Sagemaker built-in Image Classifier hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html\n",
    "\n",
    "# These hyperparameters we won't want to change, as they define things like\n",
    "# the size of the images we'll be sending for input, the number of training classes we have, etc.\n",
    "base_hyperparameters=dict(\n",
    "    use_pretrained_model=1,\n",
    "    image_shape='3,224,224',\n",
    "    num_classes=num_classes,\n",
    "    num_training_samples=num_training_samples,\n",
    ")\n",
    "\n",
    "# These are hyperparameters we may want to tune, as they can affect the model training success:\n",
    "hyperparameters={\n",
    "    **base_hyperparameters, \n",
    "    **dict(\n",
    "        learning_rate=0.001,\n",
    "        mini_batch_size=5,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "image_classifier.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c462aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 20:25:12 Starting - Starting the training job...\n",
      "2021-07-06 20:25:35 Starting - Launching requested ML instancesProfilerReport-1625603111: InProgress\n",
      "......\n",
      "2021-07-06 20:26:35 Starting - Preparing the instances for training............\n",
      "2021-07-06 20:28:35 Downloading - Downloading input data\n",
      "2021-07-06 20:28:35 Training - Downloading the training image......\n",
      "2021-07-06 20:29:35 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'num_classes': '3', 'num_training_samples': '21', 'use_pretrained_model': '1', 'image_shape': '3,224,224', 'learning_rate': '0.001', 'mini_batch_size': '5'}\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] Final configuration: {'use_pretrained_model': '1', 'num_layers': 152, 'epochs': 30, 'learning_rate': '0.001', 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': '5', 'image_shape': '3,224,224', 'precision_dtype': 'float32', 'num_classes': '3', 'num_training_samples': '21'}\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] use_pretrained_model: 1\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] multi_label: 0\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] Using pretrained model for initializing weights and transfer learning.\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] num_layers: 152\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] epochs: 30\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] learning_rate: 0.001\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] num_training_samples: 21\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] mini_batch_size: 5\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] num_classes: 3\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] kv_store: device\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:33 INFO 139781595309888] --------------------\u001b[0m\n",
      "\u001b[34m[20:29:33] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...\u001b[0m\n",
      "\u001b[34m[20:29:33] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:35 INFO 139781595309888] Setting number of threads: 3\u001b[0m\n",
      "\u001b[34m[20:29:44] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_ecl_Cuda_10.1.x.6753.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:53 INFO 139781595309888] Epoch[0] Train-accuracy=0.450000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:53 INFO 139781595309888] Epoch[0] Time cost=9.833\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:53 INFO 139781595309888] Epoch[0] Validation-accuracy=0.600000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:54 INFO 139781595309888] Storing the best model with validation accuracy: 0.600000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:54 INFO 139781595309888] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:56 INFO 139781595309888] Epoch[1] Train-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:56 INFO 139781595309888] Epoch[1] Time cost=1.288\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:56 INFO 139781595309888] Epoch[1] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:56 INFO 139781595309888] Storing the best model with validation accuracy: 1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:57 INFO 139781595309888] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:58 INFO 139781595309888] Epoch[2] Train-accuracy=0.850000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:58 INFO 139781595309888] Epoch[2] Time cost=1.212\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:29:59 INFO 139781595309888] Epoch[2] Validation-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:00 INFO 139781595309888] Epoch[3] Train-accuracy=0.850000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:00 INFO 139781595309888] Epoch[3] Time cost=1.227\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:01 INFO 139781595309888] Epoch[3] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:03 INFO 139781595309888] Epoch[4] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:03 INFO 139781595309888] Epoch[4] Time cost=1.340\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:03 INFO 139781595309888] Epoch[4] Validation-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:05 INFO 139781595309888] Epoch[5] Train-accuracy=0.950000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:05 INFO 139781595309888] Epoch[5] Time cost=1.250\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:06 INFO 139781595309888] Epoch[5] Validation-accuracy=0.700000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:07 INFO 139781595309888] Epoch[6] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:07 INFO 139781595309888] Epoch[6] Time cost=1.248\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:08 INFO 139781595309888] Epoch[6] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:10 INFO 139781595309888] Epoch[7] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:10 INFO 139781595309888] Epoch[7] Time cost=1.215\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:10 INFO 139781595309888] Epoch[7] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:12 INFO 139781595309888] Epoch[8] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:12 INFO 139781595309888] Epoch[8] Time cost=1.208\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:12 INFO 139781595309888] Epoch[8] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:14 INFO 139781595309888] Epoch[9] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:14 INFO 139781595309888] Epoch[9] Time cost=1.264\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:15 INFO 139781595309888] Epoch[9] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:16 INFO 139781595309888] Epoch[10] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:16 INFO 139781595309888] Epoch[10] Time cost=1.220\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:17 INFO 139781595309888] Epoch[10] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:19 INFO 139781595309888] Epoch[11] Train-accuracy=0.850000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:19 INFO 139781595309888] Epoch[11] Time cost=1.214\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:19 INFO 139781595309888] Epoch[11] Validation-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:21 INFO 139781595309888] Epoch[12] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:21 INFO 139781595309888] Epoch[12] Time cost=1.229\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:21 INFO 139781595309888] Epoch[12] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:23 INFO 139781595309888] Epoch[13] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:23 INFO 139781595309888] Epoch[13] Time cost=1.214\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:24 INFO 139781595309888] Epoch[13] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:25 INFO 139781595309888] Epoch[14] Train-accuracy=0.950000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:25 INFO 139781595309888] Epoch[14] Time cost=1.292\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:26 INFO 139781595309888] Epoch[14] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:28 INFO 139781595309888] Epoch[15] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:28 INFO 139781595309888] Epoch[15] Time cost=1.222\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:28 INFO 139781595309888] Epoch[15] Validation-accuracy=1.000000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[07/06/2021 20:30:31 INFO 139781595309888] Epoch[16] Train-accuracy=0.950000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:31 INFO 139781595309888] Epoch[16] Time cost=1.221\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:32 INFO 139781595309888] Epoch[16] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:33 INFO 139781595309888] Epoch[17] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:33 INFO 139781595309888] Epoch[17] Time cost=1.212\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:34 INFO 139781595309888] Epoch[17] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:36 INFO 139781595309888] Epoch[18] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:36 INFO 139781595309888] Epoch[18] Time cost=1.228\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:36 INFO 139781595309888] Epoch[18] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:38 INFO 139781595309888] Epoch[19] Train-accuracy=0.950000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:38 INFO 139781595309888] Epoch[19] Time cost=1.272\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:38 INFO 139781595309888] Epoch[19] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:40 INFO 139781595309888] Epoch[20] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:40 INFO 139781595309888] Epoch[20] Time cost=1.213\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:40 INFO 139781595309888] Epoch[20] Validation-accuracy=0.800000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:42 INFO 139781595309888] Epoch[21] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:42 INFO 139781595309888] Epoch[21] Time cost=1.223\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:43 INFO 139781595309888] Epoch[21] Validation-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:45 INFO 139781595309888] Epoch[22] Train-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:45 INFO 139781595309888] Epoch[22] Time cost=1.220\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:45 INFO 139781595309888] Epoch[22] Validation-accuracy=0.900000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:47 INFO 139781595309888] Epoch[23] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:47 INFO 139781595309888] Epoch[23] Time cost=1.224\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:47 INFO 139781595309888] Epoch[23] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:49 INFO 139781595309888] Epoch[24] Train-accuracy=0.950000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:49 INFO 139781595309888] Epoch[24] Time cost=1.272\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:50 INFO 139781595309888] Epoch[24] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:51 INFO 139781595309888] Epoch[25] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:51 INFO 139781595309888] Epoch[25] Time cost=1.246\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:52 INFO 139781595309888] Epoch[25] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:54 INFO 139781595309888] Epoch[26] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:54 INFO 139781595309888] Epoch[26] Time cost=1.220\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:54 INFO 139781595309888] Epoch[26] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:56 INFO 139781595309888] Epoch[27] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:56 INFO 139781595309888] Epoch[27] Time cost=1.223\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:56 INFO 139781595309888] Epoch[27] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:58 INFO 139781595309888] Epoch[28] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:58 INFO 139781595309888] Epoch[28] Time cost=1.226\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:30:58 INFO 139781595309888] Epoch[28] Validation-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:31:00 INFO 139781595309888] Epoch[29] Train-accuracy=1.000000\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:31:00 INFO 139781595309888] Epoch[29] Time cost=1.269\u001b[0m\n",
      "\u001b[34m[07/06/2021 20:31:01 INFO 139781595309888] Epoch[29] Validation-accuracy=1.000000\u001b[0m\n",
      "\n",
      "2021-07-06 20:31:16 Uploading - Uploading generated training model\n",
      "2021-07-06 20:31:45 Completed - Training job completed\n",
      "Training seconds: 209\n",
      "Billable seconds: 209\n",
      "\n",
      "\n",
      " Finished training! The model is available for download at: s3://sagemaker-us-east-2-964137047091/test/output/IC-test-1625603111/output/model.tar.gz\n",
      "CPU times: user 786 ms, sys: 57.5 ms, total: 844 ms\n",
      "Wall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "now = str(int(time.time()))\n",
    "training_job_name = 'IC-' + dataset_name.replace('_', '-') + '-' + now\n",
    "\n",
    "image_classifier.fit(inputs=data_channels, job_name=training_job_name, logs=True)\n",
    "\n",
    "job = image_classifier.latest_training_job\n",
    "model_path = f\"{base_dir}/{job.name}\"\n",
    "\n",
    "print(f\"\\n\\n Finished training! The model is available for download at: {image_classifier.output_path}/{job.name}/output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5966b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 239 ms, sys: 21 ms, total: 260 ms\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Deploying a model to an endpoint takes a few minutes to complete\n",
    "\n",
    "deployed_endpoint = image_classifier.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.t2.medium'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df9e6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_endpoint.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
